{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT\n",
    "\n",
    "This notebook is only a guide on how to fine-tune the **DistilBERT** model, because the actual training of the model took place on **Google Colab**, in order to utilize the GPU. For this reason, only some cells have been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T03:40:32.193962Z",
     "start_time": "2020-10-06T03:40:31.723690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is cuda available? False.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from thc.utils.env import check_repository_path\n",
    "\n",
    "\n",
    "REPOSITORY_DIR = check_repository_path()\n",
    "PROCESSED_DATA_DIR = REPOSITORY_DIR.joinpath(\"data\", \"processed\")\n",
    "print(\n",
    "    f\"Is cuda available? {torch.cuda.is_available()}.\"\n",
    ")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Train-valid-test datasets should be prepared according to the previous [01-train-valid-split](https://github.com/mrtovsky/thc/blob/main/notebooks/01-train-valid-split.ipynb) notebook and the final data **processed** folder structure should look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T03:40:32.829743Z",
     "start_time": "2020-10-06T03:40:32.697620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../data/processed/\u001b[00m\r\n",
      "├── \u001b[01;32mtest_tags.txt\u001b[00m\r\n",
      "├── \u001b[01;32mtest_text.txt\u001b[00m\r\n",
      "├── train_tags.txt\r\n",
      "├── train_text.txt\r\n",
      "├── valid_tags.txt\r\n",
      "└── valid_text.txt\r\n",
      "\r\n",
      "0 directories, 6 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../data/processed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T03:40:36.200268Z",
     "start_time": "2020-10-06T03:40:34.059102Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from thc import datasets\n",
    "from thc.preprocessing import TRANSFORMS\n",
    "\n",
    "\n",
    "BATCH_SIZE: int = 16\n",
    "\n",
    "\n",
    "train_tweets = datasets.TweetsDataset(\n",
    "    text_file=PROCESSED_DATA_DIR.joinpath(\"train_text.txt\"),\n",
    "    tags_file=PROCESSED_DATA_DIR.joinpath(\"train_tags.txt\"),\n",
    "    transform=TRANSFORMS,\n",
    ")\n",
    "valid_tweets = datasets.TweetsDataset(\n",
    "    text_file=PROCESSED_DATA_DIR.joinpath(\"valid_text.txt\"),\n",
    "    tags_file=PROCESSED_DATA_DIR.joinpath(\"valid_tags.txt\"),\n",
    "    transform=TRANSFORMS,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_tweets,  batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    dataset=valid_tweets,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "**Transformers** native implementation of the **DistilBERT** has been slightly modified to meet the requirements of the problem posed. The size of the output has been changed to match the 3-classes classification problem and a dropout layer has been added preceding the fully-connected layer. The pre-trained weights remained unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T03:40:39.355498Z",
     "start_time": "2020-10-06T03:40:36.233682Z"
    }
   },
   "outputs": [],
   "source": [
    "from thc.models import DistilBertClassifier\n",
    "\n",
    "\n",
    "model = DistilBertClassifier(output_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T03:33:55.598977Z",
     "start_time": "2020-10-06T03:33:49.429227Z"
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from thc.arena import run_experiment, TrainTestDataloaders\n",
    "from thc.preprocessing import TOKENIZER\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "artifacts_dir = REPOSITORY_DIR.joinpath(\"models\", \"distilbert-fine-tuning\")\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "logs_dir = REPOSITORY_DIR.joinpath(\"logs\", \"distilbert-fine-tuning\")\n",
    "logs_dir.mkdir(exist_ok=True)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=len(train_dataloader) * EPOCHS * 0.05,\n",
    "    num_training_steps=len(train_dataloader) * EPOCHS,\n",
    ")\n",
    "# Apply balanced class weights\n",
    "with codecs.open(PROCESSED_DATA_DIR.joinpath(\"train_tags.txt\"), \"r\") as file:\n",
    "    train_tags = [int(tag) for tag in file]\n",
    "class_weights = torch.from_numpy(\n",
    "    len(train_tags)\n",
    "    / (len(np.unique(train_tags)) * np.bincount(train_tags))\n",
    ").float().to(DEVICE)\n",
    "print(\"Class weights:\", class_weights)\n",
    "objective = nn.CrossEntropyLoss(weight=class_weights)\n",
    "train_test_dataloaders = TrainTestDataloaders(train=train_dataloader, test=valid_dataloader)\n",
    "writer = SummaryWriter(log_dir=logs_dir)\n",
    "\n",
    "run_experiment(\n",
    "    model=model,\n",
    "    dataloaders=train_test_dataloaders,\n",
    "    tokenizer=TOKENIZER,\n",
    "    device=DEVICE,\n",
    "    optimizer=optimizer,\n",
    "    objective=objective,\n",
    "    epochs=EPOCHS,\n",
    "    scheduler=scheduler,\n",
    "    artifacts_dir=artifacts_dir,\n",
    "    writer=writer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thc-venv",
   "language": "python",
   "name": "thc-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
